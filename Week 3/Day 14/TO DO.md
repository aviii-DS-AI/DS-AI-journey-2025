
# ✅ Day 14 – KNN: Learning From the Crowd

---

## 📘 Concepts to Learn
- [ ] What is **K-Nearest Neighbors (KNN)**?
  - [ ] Instance-based learning (no training phase!)
  - [ ] Role of “distance” in prediction
  - [ ] What does K mean? Why does it matter?
- [ ] Understand:
  - [ ] Euclidean Distance
  - [ ] Curse of Dimensionality (basic intro)
  - [ ] Effect of feature scaling on KNN

---

## ⚙️ Hands-On Practice
- [ ] Load Titanic dataset (scaled)
- [ ] Use `StandardScaler()` to normalize features
- [ ] Train `KNeighborsClassifier()` with:
  - [ ] Default K value
  - [ ] Try K = 3, 5, 7, 9
- [ ] Compare accuracy for different K
- [ ] Visualize confusion matrix for best K

---

## 🧠 Interpretation
- [ ] Which K gave the best result?
- [ ] What happens if K is too low or too high?
- [ ] Why does KNN require scaled data?

---

## 📺 Quick Boosters
- [ ] [KNN Intuition (3 mins)](https://www.youtube.com/watch?v=4HKqjENq9OU)
- [ ] [How KNN Works (Practical)](https://www.youtube.com/watch?v=HVXime0nQeI)

---

## 📓 Reflection Prompts
- What felt unique about KNN vs SVM or RF?
- Which part of it do I now remember confidently?
- Could I explain KNN to my younger sibling?

---

## 🌱 Bonus Goals (Optional)
- [ ] Use `GridSearchCV` to find best `k`
- [ ] Plot a 2D toy KNN decision boundary using `make_moons()`
- [ ] Try classification on new dataset like Iris

---
