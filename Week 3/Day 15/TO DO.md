# ✅ Day 15 – Naive Bayes: Predict Like a Mathematician

---

## 📘 Concepts to Learn
- [ ] What is **Naive Bayes Classification**?
  - [ ] Based on **Bayes’ Theorem**
  - [ ] Assumes features are independent (the “naive” part)
- [ ] Understand:
  - [ ] Prior vs Likelihood vs Posterior
  - [ ] When Naive Bayes performs well
  - [ ] Why it's good for text data

---

## ⚙️ Hands-On Practice
- [ ] Load a dataset with text (or fake it with dummy labels)
  - Suggested: SMS Spam Collection or create your own list
- [ ] Preprocess using:
  - [ ] Tokenization + `CountVectorizer`
  - [ ] Optional: `TfidfVectorizer` for weighted word importance
- [ ] Train:
  - [ ] `MultinomialNB()` from `sklearn.naive_bayes`
- [ ] Predict and Evaluate:
  - [ ] Accuracy
  - [ ] Confusion Matrix
  - [ ] Classification Report

---

## 💬 Interpretability
- [ ] Print top 10 words most associated with spam
- [ ] Try predicting on a few custom text samples
- [ ] Discuss any misclassifications

---

## 📺 Quick Boosters
- [ ] [Naive Bayes Intuition](https://www.youtube.com/watch?v=O2L2Uv9pdDA)
- [ ] [Text Classification with Naive Bayes](https://www.youtube.com/watch?v=EGKeC2S44Rs)

---

## 📓 Reflection Prompts
- What part of Naive Bayes clicked with you?
- Does the “independence assumption” always work?
- Would you trust Naive Bayes in real-world text problems?

---

## 🌱 Bonus Exploration
- [ ] Compare `CountVectorizer` vs `TfidfVectorizer`
- [ ] Test model on your own WhatsApp chat export (fun!)
- [ ] Try `GaussianNB()` on numeric data (just for practice)

---


